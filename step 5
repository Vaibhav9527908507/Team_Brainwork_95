# 4. Model Training and Evaluation
X = np.array(df['document_vector'].tolist())
y = df['target']

# Addressing class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Hyperparameter tuning for RandomForestClassifier (example)
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10]
}

# Use GridSearchCV to find the best hyperparameters for RandomForestClassifier
# ... (Code for GridSearchCV with RandomForestClassifier) ...

# After tuning, create the RandomForestClassifier model with the best parameters
classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # Replace with best parameters
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

# Evaluate performance with multiple metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
